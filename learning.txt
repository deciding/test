root object:uppermost level of mapping
	typename
		properties
			field
				type
				index(analyzed,not_analyzed,no)
				analyzer
		_source
			enabled:true/false(it is a stored field, will waste disk space)
		_all
			enabled:true/false
			-->include_in_all
			{
			    "my_type": {
			        "include_in_all": false,
			        "properties": {
			            "title": {
			                "type":           "string",
			                "include_in_all": true
			            },
			            ...
			        }
			    }
			}
		_id,_type,_index,_uid settings for each doc
			_id,_index does not exist(not stored/retrievable, not indexed/searchable)
			_type indexed not stored,
			_uid stored and searchable

		#dynamic mapping
		dynamic strict
		properties
			fieldname(can add new field inside)
				type object
				dynamic true

		#customize dynamic mapping
		date_detection false

		"dynamic_templates": [
                { "es": {
                      "match":              "*_es", #fieldname match, first come just match
                      "match_mapping_type": "string", #use default mapping
                      "mapping": {
                          "type":           "string",
                          "analyzer":       "spanish"
                      }
                }},
                { "en": {
                      "match":              "*", 
                      "match_mapping_type": "string",
                      "mapping": {
                          "type":           "string",
                          "analyzer":       "english"
                      }
                }}
            ]
        not only "match", we can use "path_match", "unmatch", "path_unmatch"

#default mapping
{
    "mappings": {
        "_default_": { # only for types come after it
            "_all": { "enabled":  false }
        },
        "blog": {
            "_all": { "enabled":  true  }
        }
    }
}

#reindex
to change the field, we cannot edit them, otherwisethe index would break
bcoz of _source, we don't need to query db again
use scroll&bulk
GET /old_index/_search?scroll=1m
use timestamp to split tasks
{
    "query": {
        "range": {
            "date": {
                "gte":  "2014-01-01",
                "lt":   "2014-02-01"
            }
        }
    },
    "sort": ["_doc"],
    "size":  1000
}

#index alias and zero downtime
PUT /my_index_v1 
PUT /my_index_v1/_alias/my_index 

GET /*/_alias/my_index
GET /my_index_v1/_alias/*

reindex to my_index_v2

POST /_aliases #atomic operation
{
    "actions": [
        { "remove": { "index": "my_index_v1", "alias": "my_index" }},
        { "add":    { "index": "my_index_v2", "alias": "my_index" }}
    ]
}




Inside a shard

#Making Text Searchable
every field have an inverted index
in inverted index, it contains a lot of statistics data
inside that shard, the inverted index is immutable, every time we add new doc, it will need to index again

#Dynamically Updatable Indices
use more than one index in the lucene shard to avoid reindex when adding new doc
lucene use per-segment search
  segments
  commit point
  in-memory buffer
es index -> shard(lucene index)
create new doc: commit the buffer(I guess when a search request arrived) using fsync
delete/update: record in .del file in the commit point, delete these records from search result when there is new search

#Near Real-Time Search
fsync make it slow to make the new doc searchable
ES in memory indexing buffer --> new segment(refresh) in filesystem cache--> Disk
refresh happens every second
POST /_refresh 
POST /blogs/_refresh 
PUT /my_logs
{
  "settings": {
    "refresh_interval": "30s" #2m, -1
  }
}

#Making Changes Persistent
use translog to record the diffs btw two commits
it stores all the changes thats in the buffer or cache, but haven't been flushed to the disk to avoid server shutdown problems
translog is cleared after each commit(commit+clear translog=flush)
flush every 30m or translog too large
POST /blogs/_flush 
POST /_flush?wait_for_ongoing
it's better to flush before close index/node, since it need us to replay the translog when reopen the index
for fsync the request to translog, it is every 5s and after each request

#Segment Merging
otherwise will be very slow
merge segments of similar size in the background
the lazy deleted docs are not copied
merged segment at first is not searchable since not refreshed, but it will be flushed
new commit point created and old segments deleted
POST /logstash-2014-10/_optimize?max_num_segments=1 
this is perticularly useful for log, since log hardly changes
optimize is foreground while automatic merge is background










--Search in Depth
